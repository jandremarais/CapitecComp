<!DOCTYPE html>
<html>
  <head>
    <title>Loan Delinquency Prediction</title>
    <meta charset="utf-8">
    <meta name="author" content="Jandré Marais" />
    <link href="libs/remark-css/example.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Loan Delinquency Prediction
## Capitec Data Science Competition
### Jandré Marais
### 2017/09/22

---

class: inverse



# Outline

--
1. Introduction

--

2. Methodology:

    - Data Preprocessing
    - Baseline Model - `xgboost`
    - Feature Engineering
    - Final Prediction

--
3. Insights

--

4. Conclusion

---

# Introduction

--

- **Goal:** predict which clients are likely to start experiencing financial strain.

--

- Output evaluated with ranking measure, AUC.

--

- We treat it as a binary classification problem.

- Output interpreted as the likelihood of experiencing financial strain, can then rank clients.

--

- **Challenge:** We don't know the meaning of most of the variables - makes feature engineering hard!

---
class: center, middle

![](avariable.png)

---

layout: true

### Methodology

---

## Preprocessing

--

- Decide on variable types:

    - Character variables -&gt; categorical
    - Variables with 2 unique values -&gt; binary
    - The rest -&gt; continuous
    - Luckily tree based methods are quite robust to these choices.

- One-hot encode the categorical variables (dummy variables).
    
--

- Remove `RID` and `RTSV1` since they are not in the test data.

--

- Missing values:

    - For tree based methods, it's usually sufficient if the missing values are well separated from other values.
    - `Salary_Frequency` `NA`s treated as another class.
    - `AveCredit` and `AveDebit` both had many -99 values. Since `AveDebit` &lt; 0, changed its -99 to 99.

---

## Baseline Model

--

- First fit a strong baseline to which I can compare future changes.
- Chose the `xgboost` package for gradient boosted trees.

    - I have experience with boosting.
    - `xgboost` proven very effective in Kaggle competitions, especially with structured/tabular data.
    
--

- No hyperparameter tuning - too time consuming + improvements are limited.
- Chose parameters intuitively, after testing a few.

    - `max_depth = 5`, small learning rate, early stopping, subsample by column and row (0.8).
    
---

## Baseline Model

- Evaluate with 5-fold cross-validation.

    - keep the split fixed for future models, for fair comparisons.
    - 10-fold gives more accurate results, but more time consuming.
    
--
    
- Predictions on test: average of the predictions of model trained on each training subset.

.center[
![](baseline_scores.png)
]

- Would've placed me second in the competition.

---

## Feature Engineering

- process: try something, evaluate, keep or reject

---

## Feature Engineering
### Small Improvements

--

- Grouped all the 'Gov-...' employer groups together.

---
layout: false
class: center, middle

![](imgs/gov.png)

---
layout: true

### Methodology

---

## Feature Engineering
### Small Improvements

- Grouped all the 'Gov-...' employer groups together.

- Made two mining groups: 'cheap' and 'valuable'.

--

- added new feautres for:

    - percentage salary subtractions
    - percentage loans not taken up
    - Average credict vs nett salary ratio
    - split `PDU` in 3

--

- removed `RG1` because of this relationship with `IDCC`.
  
---
layout: false
class: center, middle

![](imgs/RG1.png)

---
layout: true

### Methodolgy

---

## Feature Engineering
### Big Improvements

- `TimeSinceLastLoan`

    - created a DOW, DOM, MOY features.
    - Why did it work?

---
layout: false
class: center, middle

![](imgs/DOW.png)

---
layout: true

### Methodolgy

---

## Feature Engineering
### Big Improvements

- `ADB13` and `ADB103`

    - Look at the cyclical structure.
    
---
layout: false
class: center, middle

![](imgs/ADB.png)

---
layout: true

### Methodolgy

---

## Feature Engineering
### Big Improvements

- `ADB13` and `ADB103`

    - Look at the cyclical structure.
    - Did not know how to capture it, unevenly spaced.
    - Divided by (1/92) and took the decimal values; close to 1 means close to peak.
    
---

## Feature Engineering

- Results after feature engineering:

![](adb_10fold.png)

- 1st place!

---
## Final Prediction

--

- Created two more models, `max_depth = 3` and `max_depth = 7`, trained on different cv splits.

- Weighted average

--

- Psuedo-labelling -&gt; limited improvement, probably not worth it

--

- None of these made a big difference.

--

- Final Score:

![](1st.png)

---
layout: false

# How can we use this model in practice?

- `xgboost` give very accurate results and also very efficient:

    - training time `\(\approx\)` 1.4 minutes on 31500 observations
    - prediction time `\(\approx\)` 0.48 seconds on 3500 observations
    
- However, some refer to it as a black box (magic)

- one thing we get for free is the feature importance:

---

# How can we use this model in practice?

![](imp.png)

- Tells us how many times `xgboost` found a feature useful to split on.
- Did not work as a feature selection tool.

---

# How can we use this model in practice?

Also nice to investigate this 'multi-tree' plot.

![](multitree.png)

Shows the top three most frequent splits at each node.

---

# How can we use this model in practice?

.pull-right[Compare rank with true output - quite accurate!]


```
## # A tibble: 20 x 3
##       id      prob  true
##    &lt;int&gt;     &lt;dbl&gt; &lt;int&gt;
##  1 32474 0.9883617     1
##  2 27850 0.9803508     1
##  3 26601 0.9766479     1
##  4 44780 0.9753396     1
##  5 27849 0.9732882     1
##  6 34104 0.9700568     1
##  7 39702 0.9639232     1
##  8 32342 0.9636406     0
##  9 43620 0.9516718     1
## 10 14738 0.9503720     1
## 11 11136 0.9422292     0
## 12 26002 0.9308024     1
## 13 28535 0.9226217     1
## 14 15948 0.9108235     1
## 15 32036 0.9086132     1
## 16  9123 0.9057906     1
## 17 28233 0.9001222     1
## 18  3775 0.8996380     1
## 19 38895 0.8980138     1
## 20  5251 0.8928109     1
```

---

# Conclusion

- Main contributions to solution:

    - `xgboost` 
    - feature engineering

- More possibilities if we know the meaning of the variables.

- Model is useful for deployment.

---

class: center, middle, inverse

# Thank you!

---

class: center, inverse

# Questions?

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

--

.pull-down[
## maraisjandre9@gmail.com
## https://github.com/jandremarais/CapitecComp
]
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {window.dispatchEvent(new Event('resize'));});
(function() {var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler"); if (!r) return; s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }"; d.head.appendChild(s);})();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
